{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "  # Optimizer w/ learning rate decay.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.9)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-b0900ef7635b>:4 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 2.918096\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 100: 1.564008\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 65.2%\n",
      "Minibatch loss at step 200: 1.081790\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 300: 1.328269\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 400: 1.192118\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 500: 0.639404\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 600: 1.100861\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 700: 0.561790\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 800: 0.458544\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 900: 0.713941\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 1000: 0.347145\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Test accuracy: 89.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1\n",
    "=========\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))    \n",
    "    \n",
    "    \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(pool + layer1_biases)\n",
    "            \n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(pool + layer2_biases)\n",
    "    \n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "  # Optimizer w/ learning rate decay.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.9)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.483711\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 100: 1.628864\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 63.4%\n",
      "Minibatch loss at step 200: 1.175636\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 300: 1.144591\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 74.9%\n",
      "Minibatch loss at step 400: 1.112732\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 77.7%\n",
      "Minibatch loss at step 500: 0.624896\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 600: 1.439164\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 700: 0.465617\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.9%\n",
      "Minibatch loss at step 800: 0.456498\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 900: 0.654596\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 1000: 0.312720\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 84.3%\n",
      "Test accuracy: 90.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2\n",
    "=========\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic LeNet5 architecture, adding Dropout, and/or adding learning rate decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 32#16#94.8%\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 128#64\n",
    "beta = 0.005\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Placeholder to control dropout probability.\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  keep_prob_conv = tf.placeholder(tf.float32)\n",
    "    \n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer2a_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2a_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal([4 * 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))    \n",
    "    \n",
    "    \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(pool + layer1_biases)\n",
    "\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(pool + layer2_biases)\n",
    "\n",
    "    conv = tf.nn.conv2d(hidden, layer2a_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    pool = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(pool + layer2a_biases)\n",
    "    \n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    drop = tf.nn.dropout(hidden, keep_prob)\n",
    "    \n",
    "    return tf.matmul(drop, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "\n",
    "  # Add the regularization term to the loss.\n",
    "  loss += beta * (tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer4_weights))\n",
    "\n",
    "  # Optimizer w/ learning rate decay.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(0.1, global_step, 3000, 0.01)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 6.712016\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 100: 2.595669\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 27.5%\n",
      "Minibatch loss at step 200: 1.446040\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 300: 1.594900\n",
      "Minibatch accuracy: 65.6%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 400: 1.465292\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 500: 0.907639\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 600: 0.851826\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 700: 0.757362\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 800: 0.874789\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 900: 1.230573\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.9%\n",
      "Minibatch loss at step 1000: 0.825831\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 1100: 0.834065\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 1200: 0.748064\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1300: 0.977312\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1400: 0.699148\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 1500: 0.670250\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 1600: 0.924020\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1700: 0.411786\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 1800: 0.549918\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1900: 0.831919\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2000: 0.786327\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2100: 0.754802\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2200: 0.721017\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 2300: 0.774539\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2400: 0.595340\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2500: 0.506562\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2600: 0.693725\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2700: 0.267519\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 88.0%\n",
      "Minibatch loss at step 2800: 0.271084\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2900: 0.489859\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 3000: 0.264786\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 88.3%\n",
      "Test accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "steps = np.array([])\n",
    "loss_batch = np.array([])\n",
    "acc_valid = np.array([])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob:1.0, keep_prob_conv:1.0}\n",
    "    feed_dict_w_dropout = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob:0.5, keep_prob_conv:1.0}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict_w_dropout)\n",
    "    if (step % 100 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(feed_dict=feed_dict), valid_labels))\n",
    "    \n",
    "      steps = np.append(steps, step)\n",
    "      loss_batch = np.append(loss_batch, l)\n",
    "      acc_valid = np.append(acc_valid, [accuracy(valid_prediction.eval(feed_dict=feed_dict), valid_labels)])\n",
    "    \n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(feed_dict=feed_dict), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss evolution\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAFkCAYAAACw3EhvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f54f4e4d160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Loss evolution')\n",
    "s1, = plt.plot(steps, loss_batch)\n",
    "s2, = plt.plot(steps, acc_valid/100)\n",
    "plt.legend([s1, s2], ['loss_batch','accuracy_valid'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
