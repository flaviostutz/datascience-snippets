{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train sea lion classifier with a convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../../input/kaggle-sea-lion/03/'\n",
    "OUTPUT_DIR = '../../output/kaggle-sea-lion/05/'\n",
    "IMAGE_DIMS = (84,84,3)\n",
    "\n",
    "INPUT_DATASET_NAME = 'lion-patches-0px-balanced'\n",
    "LOAD_WEIGHTS_FILE = OUTPUT_DIR + 'last-weights.h5'\n",
    "SAVE_WEIGHTS_FILE = LOAD_WEIGHTS_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from modules.logging import logger\n",
    "import modules.utils as utils\n",
    "from modules.utils import Timer\n",
    "import modules.logging\n",
    "import modules.cnn as cnn\n",
    "import modules.lions as lions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prepare output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 04:22:56,668 INFO Output dirs created\n"
     ]
    }
   ],
   "source": [
    "utils.mkdirs(OUTPUT_DIR, dirs=['tf-logs','weights'], recreate=False)\n",
    "modules.logging.setup_file_logger(OUTPUT_DIR + 'out.log')\n",
    "tf_logs_dir = OUTPUT_DIR + '/tf-logs/'\n",
    "weights_file = OUTPUT_DIR + 'weights-{epoch:02d}-{val_acc:.2f}.h5'\n",
    "logger.info('Output dirs created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prepare CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 04:22:56,676 INFO Load CNN model for training\n",
      "/notebooks/datascience-snippets/kaggle-sea-lion/modules/lions.py:50: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (11, 11), activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=\"conv_1\", padding=\"valid\")`\n",
      "  conv_1 = convolutional.Convolution2D(96, 11, 11, border_mode='valid', name=\"conv_1\", activation='relu', init='glorot_uniform')(input)\n",
      "/notebooks/datascience-snippets/kaggle-sea-lion/modules/lions.py:53: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=\"conv_2\", padding=\"valid\")`\n",
      "  conv_2 = convolutional.Convolution2D(256, 3, 3, border_mode='valid', name=\"conv_2\", activation='relu', init='glorot_uniform')(zero_padding_1)\n",
      "/notebooks/datascience-snippets/kaggle-sea-lion/modules/lions.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(384, (3, 3), activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=\"conv_3\", padding=\"valid\")`\n",
      "  conv_3 = convolutional.Convolution2D(384, 3, 3, border_mode='valid', name=\"conv_3\", activation='relu', init='glorot_uniform')(zero_padding_2)\n",
      "/notebooks/datascience-snippets/kaggle-sea-lion/modules/lions.py:57: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(384, (3, 3), activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=\"conv_4\", padding=\"valid\")`\n",
      "  conv_4 = convolutional.Convolution2D(384, 3, 3, border_mode='valid', name=\"conv_4\", activation='relu', init='glorot_uniform')(conv_3)\n",
      "/notebooks/datascience-snippets/kaggle-sea-lion/modules/lions.py:58: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=\"conv_5\", padding=\"valid\")`\n",
      "  conv_5 = convolutional.Convolution2D(256, 3, 3, border_mode='valid', name=\"conv_5\", activation='relu', init='glorot_uniform')(conv_4)\n",
      "/notebooks/datascience-snippets/kaggle-sea-lion/modules/lions.py:61: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4096, activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=\"fc_1\")`\n",
      "  fc_1 = core.Dense(4096, name=\"fc_1\", activation='relu', init='glorot_uniform')(flatten)\n",
      "/notebooks/datascience-snippets/kaggle-sea-lion/modules/lions.py:63: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(4096, activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=\"Output\")`\n",
      "  output = core.Dense(4096, name=\"Output\", activation='relu', init='glorot_uniform')(fc_1)\n",
      "/notebooks/datascience-snippets/kaggle-sea-lion/modules/lions.py:65: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"softmax\", kernel_initializer=\"glorot_uniform\", name=\"fc_2\")`\n",
      "  fc_2 = core.Dense(NR_CLASSES, name=\"fc_2\", activation='softmax', init='glorot_uniform')(output)\n"
     ]
    }
   ],
   "source": [
    "logger.info('Load CNN model for training')\n",
    "model = lions.convnet_alexnet_lion_keras(IMAGE_DIMS)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-10 04:22:56,890 INFO Using dataset ../../input/kaggle-sea-lion/03/lion-patches-0px-balanced-84-84.h5 as input\n",
      "2017-04-10 04:22:56,892 INFO loading input data\n",
      "2017-04-10 04:23:03,793 INFO X shape (37462, 84, 84, 3)\n",
      "2017-04-10 04:23:03,794 INFO Y shape (37462, 6)\n",
      "2017-04-10 04:23:03,796 INFO Starting CNN training...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fd682bef8aae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting CNN training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     history = model.fit_generator(datagenerator.flow(X_train, Y_train, batch_size = 16, shuffle = False),\n\u001b[0m\u001b[1;32m     33\u001b[0m                          \u001b[0msamples_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                          \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/Keras-2.0.2-py3.4.egg/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mflow\u001b[0;34m(self, x, y, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0msave_to_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_to_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             save_format=save_format)\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     def flow_from_directory(self, directory,\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/Keras-2.0.2-py3.4.egg/keras/preprocessing/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, seed, data_format, save_to_dir, save_prefix, save_format)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mdata_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_path = INPUT_DIR + utils.dataset_name(INPUT_DATASET_NAME, IMAGE_DIMS)\n",
    "logger.info('Using dataset ' + dataset_path + ' as input')\n",
    "\n",
    "datagenerator = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=True,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=360,\n",
    "        width_shift_range=0,\n",
    "        height_shift_range=0,\n",
    "        horizontal_flip=False,\n",
    "        vertical_flip=False)\n",
    "\n",
    "with h5py.File(dataset_path, 'r') as hdf5:\n",
    "    logger.info('loading input data')\n",
    "    X_train,Y_train = utils.dataset_xy_range(hdf5, 0, 0.8)\n",
    "    X_validation,Y_validation = utils.dataset_xy_range(hdf5, 0.8, 0.9)\n",
    "    logger.info('X shape ' + str(X_train.shape))\n",
    "    logger.info('Y shape ' + str(Y_train.shape))\n",
    "\n",
    "    if(os.path.isfile(LOAD_WEIGHTS_FILE)):\n",
    "        logger.info('Loading previous weights...')\n",
    "        model.load_weights(WEIGHTS_FILE)\n",
    "\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=tf_logs_dir, histogram_freq=0, write_graph=True, write_images=True)\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(weights_file, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    progbar_callback = keras.callbacks.ProgbarLogger(count_mode='samples')\n",
    "    \n",
    "    logger.info('Starting CNN training...')\n",
    "    history = model.fit_generator(datagenerator.flow(X_train, Y_train, batch_size = 16, shuffle = False),\n",
    "                         samples_per_epoch = len(X_train), \n",
    "                         nb_epoch = 1, \n",
    "                         callbacks = [tensorboard_callback, checkpoint_callback, progbar_callback],\n",
    "                         validation_data = (X_validation, Y_validation), \n",
    "                         verbose = 1, \n",
    "                         show_accuracy = True)    \n",
    "\n",
    "    if(SAVE_WEIGHTS_FILE!=None):\n",
    "        logger.info('Saving last weights...')\n",
    "        model.save_weights(SAVE_WEIGHTS_FILE)\n",
    "    \n",
    "    cnn.show_training_info_keras(history)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logger.info('Evaluate dataset')\n",
    "dataset_path = INPUT_DIR + utils.dataset_name('lion-patches', IMAGE_DIMS)\n",
    "\n",
    "with h5py.File(dataset_path, 'r') as hdf5:\n",
    "    X_test,Y_test = utils.dataset_xy_range(hdf5, 0.9, 1)\n",
    "    cnn.evaluate_dataset_keras(X, Y, model, batch_size=24, detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
