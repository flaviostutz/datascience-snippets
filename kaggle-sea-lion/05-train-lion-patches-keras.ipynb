{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train sea lion classifier with a convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../../input/kaggle-sea-lion/02/'\n",
    "OUTPUT_DIR = '../../output/kaggle-sea-lion/05/'\n",
    "#IMAGE_DIMS = (148,148,3)\n",
    "IMAGE_DIMS = (84,84,3)\n",
    "#IMAGE_DIMS = (32,32,3)\n",
    "\n",
    "INPUT_DATASET_NAME = 'lion-patches-0px'\n",
    "SAVE_WEIGHTS_FILE = OUTPUT_DIR + 'last-weights.h5'\n",
    "SAVE_MODEL_FILE = OUTPUT_DIR + 'last-model.yml'\n",
    "LOAD_WEIGHTS_FILE = None#OUTPUT_DIR + 'weights-0.72-simple.h5'\n",
    "LOAD_MODEL_FILE = None\n",
    "\n",
    "RECREATE_OUTPUT_DIR = False\n",
    "RUN_TRAINING = True\n",
    "\n",
    "OUTPUT_WEIGHT = (1,1,1,1,1,1)\n",
    "TRAIN_EPOCHS = 50\n",
    "INPUT_RANGE = 1\n",
    "\n",
    "BATCH_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import models\n",
    "\n",
    "from modules.logging import logger\n",
    "import modules.utils as utils\n",
    "from modules.utils import Timer\n",
    "import modules.logging\n",
    "import modules.cnn as cnn\n",
    "import modules.lions as lions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prepare output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-23 21:42:00,483 INFO Output dirs created\n"
     ]
    }
   ],
   "source": [
    "utils.mkdirs(OUTPUT_DIR, dirs=['tf-logs','weights'], recreate=RECREATE_OUTPUT_DIR)\n",
    "modules.logging.setup_file_logger(OUTPUT_DIR + 'out.log')\n",
    "TF_LOGS_DIR = OUTPUT_DIR + 'tf-logs/'\n",
    "WEIGHTS_DIR = OUTPUT_DIR + 'weights/'\n",
    "input_dataset_path = INPUT_DIR + utils.dataset_name(INPUT_DATASET_NAME, IMAGE_DIMS)\n",
    "\n",
    "logger.info('Output dirs created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prepare train, validate and test data flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-23 21:42:00,515 INFO Using dataset ../../input/kaggle-sea-lion/02/lion-patches-0px-84-84.h5 as input\n",
      "2017-04-23 21:42:00,517 INFO preparing train data\n",
      "2017-04-23 21:42:00,518 INFO loading input data for class distribution analysis...\n",
      "2017-04-23 21:42:00,519 INFO loading Y from raw dataset\n",
      "2017-04-23 21:42:00,520 INFO > [started] generator dump...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135062/135062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-23 21:42:44,797 INFO > [done]    generator dump (44277.134 ms)\n",
      "2017-04-23 21:42:44,858 INFO raw sample class distribution\n",
      "2017-04-23 21:42:44,859 INFO 0: 4930\n",
      "2017-04-23 21:42:44,860 INFO 1: 3902\n",
      "2017-04-23 21:42:44,860 INFO 2: 33128\n",
      "2017-04-23 21:42:44,861 INFO 3: 17819\n",
      "2017-04-23 21:42:44,862 INFO 4: 14882\n",
      "2017-04-23 21:42:44,863 INFO 5: 60401\n",
      "2017-04-23 21:42:44,864 INFO overall output samples per class: 23412\n",
      "2017-04-23 21:42:44,865 INFO augmentation/undersampling ratio per class\n",
      "2017-04-23 21:42:44,866 INFO SETUP FLOW 0 0.7\n",
      "2017-04-23 21:42:44,867 INFO calculating source range according to start/end range of the desired output..\n",
      "2017-04-23 21:42:44,868 INFO output distribution for this flow\n",
      "2017-04-23 21:42:44,869 INFO 0: 16388 (4.75)\n",
      "2017-04-23 21:42:44,869 INFO 1: 16388 (6.00)\n",
      "2017-04-23 21:42:44,870 INFO 2: 16388 (0.71)\n",
      "2017-04-23 21:42:44,871 INFO 3: 16388 (1.31)\n",
      "2017-04-23 21:42:44,872 INFO 4: 16388 (1.57)\n",
      "2017-04-23 21:42:44,873 INFO 5: 16388 (0.39)\n",
      "2017-04-23 21:42:45,013 INFO source range: 0-94164 (94164)\n",
      "2017-04-23 21:42:45,015 INFO output range: 0-98330 (98330)\n",
      "2017-04-23 21:42:45,016 INFO train size=98330 batches=1537\n",
      "2017-04-23 21:42:45,017 INFO preparing valid data\n",
      "2017-04-23 21:42:45,018 INFO loading input data for class distribution analysis...\n",
      "2017-04-23 21:42:45,019 INFO loading Y from raw dataset\n",
      "2017-04-23 21:42:45,020 INFO > [started] generator dump...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89920/135062"
     ]
    }
   ],
   "source": [
    "logger.info('Using dataset ' + input_dataset_path + ' as input')\n",
    "h5file = h5py.File(input_dataset_path, 'r')\n",
    "\n",
    "#used for image augmentation (creating new images for balancing)\n",
    "image_augmentation_generator = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=False,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=False,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=359,\n",
    "        width_shift_range=0,\n",
    "        height_shift_range=0,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "\n",
    "#applied to all images during training\n",
    "image_randomize_generator = ImageDataGenerator(\n",
    "        featurewise_center=False,\n",
    "        samplewise_center=True,\n",
    "        featurewise_std_normalization=False,\n",
    "        samplewise_std_normalization=True,\n",
    "        zca_whitening=False,\n",
    "        rotation_range=359,\n",
    "        width_shift_range=6,\n",
    "        height_shift_range=6,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True)\n",
    "\n",
    "logger.info('preparing train data')\n",
    "train_batch_generator = utils.BatchGeneratorXYH5(h5file, start_ratio=0, end_ratio=INPUT_RANGE)\n",
    "train_balance_generator = utils.ClassBalancerGeneratorXY(train_batch_generator,\n",
    "                                                         image_augmentation=image_augmentation_generator,\n",
    "                                                         output_weight=OUTPUT_WEIGHT,\n",
    "                                                         max_augmentation_ratio=5,\n",
    "                                                         max_undersampling_ratio=1,\n",
    "                                                         enforce_max_ratios=False,\n",
    "                                                         start_ratio=0, end_ratio=0.7)\n",
    "train_generator = utils.image_augmentation_xy(train_balance_generator.flow(), image_randomize_generator)\n",
    "logger.info('train size=' + str(train_balance_generator.size) + ' batches=' + str(train_balance_generator.nr_batches))\n",
    "\n",
    "\n",
    "logger.info('preparing valid data')\n",
    "valid_batch_generator = utils.BatchGeneratorXYH5(h5file, start_ratio=0, end_ratio=INPUT_RANGE)\n",
    "valid_balance_generator = utils.ClassBalancerGeneratorXY(valid_batch_generator,\n",
    "                                                         image_augmentation=image_augmentation_generator,\n",
    "                                                         output_weight=OUTPUT_WEIGHT,\n",
    "                                                         max_augmentation_ratio=5,\n",
    "                                                         max_undersampling_ratio=1,\n",
    "                                                         enforce_max_ratios=False,\n",
    "                                                         start_ratio=0.7, end_ratio=0.85)\n",
    "logger.info('valid size=' + str(valid_balance_generator.size) + ' batches=' + str(valid_balance_generator.nr_batches))\n",
    "\n",
    "\n",
    "\n",
    "logger.info('preparing test data')\n",
    "test_batch_generator = utils.BatchGeneratorXYH5(h5file, start_ratio=0, end_ratio=INPUT_RANGE)\n",
    "test_balance_generator = utils.ClassBalancerGeneratorXY(test_batch_generator,\n",
    "                                                         image_augmentation=image_augmentation_generator,\n",
    "                                                         output_weight=OUTPUT_WEIGHT,\n",
    "                                                         max_augmentation_ratio=5,\n",
    "                                                         max_undersampling_ratio=1,\n",
    "                                                         enforce_max_ratios=False,\n",
    "                                                         start_ratio=0.85, end_ratio=1)\n",
    "logger.info('test size=' + str(test_balance_generator.size) + ' batches=' + str(test_balance_generator.nr_batches))\n",
    "\n",
    "#FIXME when using 1 on end ratio size and nr_batches gets negative (h5 batch generator, not balancer...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#logger.info('INPUT DATASET DATA')\n",
    "#dataset_path = INPUT_DIR + utils.dataset_name(INPUT_DATASET_NAME, IMAGE_DIMS)\n",
    "#with h5py.File(input_dataset_path, 'r') as h5file:\n",
    "#    logger.info('generator')\n",
    "#    input_generator = utils.BatchGeneratorXYH5(h5file, start_ratio=0.001, end_ratio=0.0012, batch_size=64)\n",
    "#    X, Y = utils.dump_xy_to_array(input_generator.flow(), input_generator.size, x=True, y=True)\n",
    "#    utils.show_images(X, image_labels=utils.onehot_to_label(Y), group_by_label=False, cols=10, is_bgr=True, size=2)\n",
    "#\n",
    "#    logger.info('x ' + str(np.shape(X)))\n",
    "#    logger.info('y ' + str(np.shape(Y)))\n",
    "#    logger.info(str(utils.class_distribution(Y)))\n",
    "\n",
    "logger.info('BALANCE GENERATOR DATA')\n",
    "#dataset_path = INPUT_DIR + utils.dataset_name(INPUT_DATASET_NAME, IMAGE_DIMS)\n",
    "#X_train, Y_train = utils.dump_xy_to_array(train_generator, train_balance_generator.size, x=False, y=True)\n",
    "#logger.info('y ' + str(np.shape(Y_train)))\n",
    "# logger.info(str(utils.class_distribution(Y_train)))\n",
    "\n",
    "for xs,ys in train_balance_generator.flow():\n",
    "    utils.show_images(xs, image_labels=utils.onehot_to_label(ys), cols=10, is_bgr=True, size=2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prepare CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logger.info('Load CNN model')\n",
    "#model = lions.convnet_alexnet2_lion_keras(IMAGE_DIMS)\n",
    "\n",
    "model = None\n",
    "if(LOAD_MODEL_FILE!=None and os.path.isfile(LOAD_MODEL_FILE)):\n",
    "    with open(LOAD_MODEL_FILE, 'r') as model_file:\n",
    "        my = model_file.read()\n",
    "        model = models.model_from_yaml(my)\n",
    "        logger.info('loaded model from file ' + LOAD_MODEL_FILE)\n",
    "else:\n",
    "    model = lions.convnet_simple_lion_keras(IMAGE_DIMS)\n",
    "    logger.info('loaded model from function simple')\n",
    "    \n",
    "\n",
    "if(LOAD_WEIGHTS_FILE!=None and os.path.isfile(LOAD_WEIGHTS_FILE)):\n",
    "    model.load_weights(LOAD_WEIGHTS_FILE)\n",
    "    logger.info('Loaded model weights from ' + LOAD_WEIGHTS_FILE)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])    \n",
    "logger.info('Model prepared')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if(RUN_TRAINING):\n",
    "    logger.info('Starting CNN training...')\n",
    "    history = model.fit_generator(train_generator,\n",
    "                  steps_per_epoch = train_balance_generator.nr_batches,\n",
    "                  nb_epoch = TRAIN_EPOCHS,\n",
    "                  callbacks = cnn.get_callbacks_keras(model, WEIGHTS_DIR, TF_LOGS_DIR),\n",
    "                  validation_data = valid_balance_generator.flow(), \n",
    "                  validation_steps = valid_balance_generator.nr_batches,\n",
    "                  verbose = 1)\n",
    "\n",
    "    if(SAVE_MODEL_FILE!=None):\n",
    "        with open(SAVE_MODEL_FILE, 'w') as model_file:\n",
    "            model_file.write(model.to_yaml())\n",
    "            logger.info('Saved last model to ' + SAVE_MODEL_FILE)\n",
    "    \n",
    "    if(SAVE_WEIGHTS_FILE!=None):\n",
    "        model.save_weights(SAVE_WEIGHTS_FILE)\n",
    "        logger.info('Saved last weights to ' + SAVE_WEIGHTS_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Epoch accuracy/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if(RUN_TRAINING):\n",
    "    logger.info('Training info')\n",
    "    cnn.show_training_info_keras(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn.evaluate_dataset_keras(test_balance_generator.flow(), \n",
    "                       test_balance_generator.nr_batches, \n",
    "                       test_balance_generator.size, \n",
    "                       model, \n",
    "                       class_labels=lions.CLASS_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = test_balance_generator.flow()\n",
    "cnn.show_predictions(a, 50, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
