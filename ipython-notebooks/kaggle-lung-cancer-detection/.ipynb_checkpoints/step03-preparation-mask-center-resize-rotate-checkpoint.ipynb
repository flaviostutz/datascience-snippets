{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This script takes raw dicom patient images, applies various transformations on 3d scan figure and saves it to one h5 matrix per shard. Step4 will merge it all.\n",
    "#Saved values are in HU scale\n",
    "IMAGE_DIMS = (312, 212, 312, 1)\n",
    "\n",
    "NR_SHARDS = 700\n",
    "RANDOM_SEED = 0.1\n",
    "SAVE_IMAGES = True\n",
    "\n",
    "#Patient DICOM images folder\n",
    "INPUT_FOLDER = '../../../input/stage1_images/'\n",
    "LABELS_FILE = '../../../input/stage1_labels.csv'\n",
    "\n",
    "BASE_OUTPUT_FOLDER = '../../../output/kaggle-bowl/step3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "from numpy import ndarray\n",
    "from random import shuffle\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from modules.logging import logger\n",
    "import modules.logging\n",
    "import modules.lungprepare as lungprepare\n",
    "import modules.utils as utils\n",
    "from modules.utils import Timer\n",
    "import modules.logging\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_patient_ids(shard_id, input_dir):\n",
    "    shard_patients = []\n",
    "    \n",
    "    file = csv.DictReader(open(LABELS_FILE))    \n",
    "    for row in file:\n",
    "        p = row['id']\n",
    "        if(int(p,16)%NR_SHARDS == (shard_id-1)):\n",
    "            shard_patients.append(p)\n",
    "    logger.info('found {} patients for shard {}'.format(len(shard_patients), shard_id))\n",
    "    shuffle(shard_patients, lambda: RANDOM_SEED)\n",
    "    return shard_patients\n",
    "\n",
    "#     force ids\n",
    "#     return ['0c37613214faddf8701ca41e6d43f56e', '0a0c32c9e08cc2ea76a71649de56be6d', '0a38e7597ca26f9374f8ea2770ba870d']\n",
    "#     return ['16377fe7caf072d882f234dbbff9ef6c']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def patient_label(input_dir, patient_id):\n",
    "    labels = pd.read_csv(LABELS_FILE, index_col=0)\n",
    "    value = labels.get_value(patient_id, 'cancer')\n",
    "    #one-hot encoding\n",
    "    label = np.array([0,1])\n",
    "    if(value == 0): label = np.array([1,0])\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def start_processing(input_dir, shard_id, max_patients, image_dims, base_output_dir):\n",
    "    t = Timer('shard', debug=False)\n",
    "    output_dir = base_output_dir + str(shard_id) + '/'\n",
    "    logger.info('Processing patients. shard_id=' + str(shard_id) + ' max_patients='+ str(max_patients) + ' input_dir=' + input_dir + ' output_dir=' + output_dir)\n",
    "    \n",
    "    #check if this shard was already processed\n",
    "    file_done = output_dir + 'done'\n",
    "    if(os.path.isfile(file_done)):\n",
    "        logger.warning('Shard ' + str(shard_id) + ' already processed. Skipping it.')\n",
    "        return 'shard ' + str(shard_id) + ': SKIPPED'\n",
    "\n",
    "    logger.info('Gathering patient ids for this shard')\n",
    "    patient_ids = get_patient_ids(shard_id, input_dir)\n",
    "    total_patients = len(patient_ids)\n",
    "    \n",
    "    dataset_name = 'data-centered-rotated'\n",
    "\n",
    "    logger.info('Preparing output dir')\n",
    "    utils.mkdirs(output_dir, dirs=['images'], recreate=True)\n",
    "\n",
    "    modules.logging.setup_file_logger(base_output_dir + 'out.log')\n",
    "\n",
    "    logger.info('Creating datasets')\n",
    "    dataset_file = utils.dataset_path(output_dir, dataset_name, image_dims)\n",
    "    with h5py.File(dataset_file, 'w') as h5f:\n",
    "        x_ds = h5f.create_dataset('X', (total_patients, image_dims[0], image_dims[1], image_dims[2], image_dims[3]), chunks=(1, image_dims[0], image_dims[1], image_dims[2], image_dims[3]), dtype='f')\n",
    "        y_ds = h5f.create_dataset('Y', (total_patients, 2), dtype='f')\n",
    "\n",
    "        logger.info('Starting to process each patient (count={})'.format(len(patient_ids)))\n",
    "        count = 0\n",
    "        record_row = 0\n",
    "\n",
    "        for patient_id in patient_ids:\n",
    "            if(count>(max_patients-1)):\n",
    "                break\n",
    "\n",
    "            t = Timer('>>> PATIENT PROCESSING ' + patient_id + ' (count=' + str(count) + '; output_dir=' + output_dir + ')')\n",
    "            patient_pixels = lungprepare.process_patient_images(input_dir + patient_id, image_dims, output_dir, patient_id)\n",
    "            if(patient_pixels != None):\n",
    "                if(not np.any(patient_pixels)):\n",
    "                    logger.error('Patient pixels returned with zero values patient_id=' + patient_id)\n",
    "                logger.info('Recording patient pixels to output dataset count=' + str(count))\n",
    "                x_ds[record_row] = patient_pixels\n",
    "                label = patient_label(input_dir, patient_id)\n",
    "                y_ds[record_row] = label\n",
    "                record_row = record_row + 1\n",
    "            else:\n",
    "                logger.warning('Patient lung not found. Skipping.')   \n",
    "\n",
    "            t.stop()\n",
    "            count = count + 1\n",
    "\n",
    "    if(not utils.validate_dataset(output_dir, dataset_name, image_dims, save_dir=output_dir + 'images/')):\n",
    "        logger.error('Validation ERROR on shard ' + str(shard_id))\n",
    "        return 'shard ' + str(shard_id) + ': ERROR ' + str(t.elapsed()*1000) + 'ms'\n",
    "\n",
    "    logger.info('Marking shard as processed')\n",
    "    f = open(file_done, 'w')\n",
    "    f.write('OK')\n",
    "    f.close()\n",
    "    \n",
    "    return 'shard ' + str(shard_id) + ': OK ' + str(t.elapsed()*1000) + 'ms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-12 23:36:57,486 INFO ==== PROCESSING SHARDS IN PARALLEL ====\n",
      "2017-03-12 23:36:57,488 INFO Using 1 parallel tasks\n",
      "2017-03-12 23:37:13,514 INFO Processing patients. shard_id=626 max_patients=999 input_dir=../../../input/stage1_images/ output_dir=../../../output/kaggle-bowl/step3/626/\n",
      "2017-03-12 23:37:13,517 INFO Gathering patient ids for this shard\n",
      "2017-03-12 23:37:13,522 INFO found 0 patients for shard 626\n",
      "2017-03-12 23:37:13,523 INFO Preparing output dir\n",
      "2017-03-12 23:37:13,524 INFO Creating datasets\n",
      "2017-03-12 23:37:13,526 WARNING Exception while processing shard 626: Chunk shape must not be greater than data shape in any dimension. (1, 312, 212, 312, 1) is not compatible with (0, 312, 212, 312, 1)\n",
      "2017-03-12 23:37:13,533 INFO ==== ALL DONE ====\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shard 626 exception: Chunk shape must not be greater than data shape in any dimension. (1, 312, 212, 312, 1) is not compatible with (0, 312, 212, 312, 1)\n"
     ]
    }
   ],
   "source": [
    "logger.info('==== PROCESSING SHARDS IN PARALLEL ====')\n",
    "\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "def process_shard(shard_id):\n",
    "    try:\n",
    "       sleep(randint(0,20))\n",
    "       return start_processing(INPUT_FOLDER, shard_id, 999, IMAGE_DIMS, BASE_OUTPUT_FOLDER)\n",
    "    except BaseException as e:\n",
    "       logger.warning('Exception while processing shard ' + str(shard_id) + ': ' + str(e))\n",
    "       return 'shard ' + str(shard_id) + ' exception: ' + str(e)\n",
    "\n",
    "#mp.set_start_method('spawn')\n",
    "n_processes = mp.cpu_count()\n",
    "#n_processes = 1\n",
    "logger.info('Using ' + str(n_processes) + ' parallel tasks')\n",
    "\n",
    "with Pool(n_processes) as p:\n",
    "    shards = list(range(1,NR_SHARDS+1))\n",
    "    shuffle(shards)\n",
    "#     shards = [23]\n",
    "    #http://stackoverflow.com/questions/26520781/multiprocessing-pool-whats-the-difference-between-map-async-and-imap\n",
    "    for i in p.imap_unordered(process_shard, shards):\n",
    "        print(i)\n",
    "\n",
    "logger.info('==== ALL DONE ====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
