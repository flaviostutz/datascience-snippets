{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train nodule detector with LUNA16 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_DIR = '../../input/luna16/'\n",
    "OUTPUT_DIR = '../../output/lung-cancer/01/'\n",
    "IMAGE_DIMS = (50,50,50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns \n",
    "import sklearn\n",
    "import os\n",
    "import glob\n",
    "#from PIL import Image\n",
    "\n",
    "from modules.logging import logger\n",
    "import modules.utils as utils\n",
    "from modules.utils import Timer\n",
    "import modules.logging\n",
    "import modules.cnn as cnn\n",
    "import modules.ctscan as ctscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Let us import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "annotations = pd.read_csv('../../input/luna16/annotations.csv')\n",
    "candidates = pd.read_csv('../../input/luna16/candidates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860\n",
      "                                           seriesuid      coordX      coordY  \\\n",
      "0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222... -128.699421 -175.319272   \n",
      "1  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  103.783651 -211.925149   \n",
      "2  1.3.6.1.4.1.14519.5.2.1.6279.6001.100398138793...   69.639017 -140.944586   \n",
      "3  1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...  -24.013824  192.102405   \n",
      "4  1.3.6.1.4.1.14519.5.2.1.6279.6001.100621383016...    2.441547  172.464881   \n",
      "\n",
      "       coordZ  diameter_mm  \n",
      "0 -298.387506     5.651471  \n",
      "1 -227.121250     4.224708  \n",
      "2  876.374496     5.786348  \n",
      "3 -391.081276     8.143262  \n",
      "4 -405.493732    18.545150  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1186 entries, 0 to 1185\n",
      "Data columns (total 5 columns):\n",
      "seriesuid      1186 non-null object\n",
      "coordX         1186 non-null float64\n",
      "coordY         1186 non-null float64\n",
      "coordZ         1186 non-null float64\n",
      "diameter_mm    1186 non-null float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 46.4+ KB\n"
     ]
    }
   ],
   "source": [
    "print(annotations.iloc[1]['seriesuid'])\n",
    "print(str(annotations.head()))\n",
    "annotations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222365663678666836860\n",
      "                                           seriesuid  coordX  coordY  coordZ  \\\n",
      "0  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  -56.08  -67.85 -311.92   \n",
      "1  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...   53.21 -244.41 -245.17   \n",
      "2  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  103.66 -121.80 -286.62   \n",
      "3  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  -33.66  -72.75 -308.41   \n",
      "4  1.3.6.1.4.1.14519.5.2.1.6279.6001.100225287222...  -32.25  -85.36 -362.51   \n",
      "\n",
      "   class  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n",
      "4      0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 551065 entries, 0 to 551064\n",
      "Data columns (total 5 columns):\n",
      "seriesuid    551065 non-null object\n",
      "coordX       551065 non-null float64\n",
      "coordY       551065 non-null float64\n",
      "coordZ       551065 non-null float64\n",
      "class        551065 non-null int64\n",
      "dtypes: float64(3), int64(1), object(1)\n",
      "memory usage: 21.0+ MB\n"
     ]
    }
   ],
   "source": [
    "print(candidates.iloc[1]['seriesuid'])\n",
    "print(str(candidates.head()))\n",
    "candidates.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351\n",
      "549714\n"
     ]
    }
   ],
   "source": [
    "print(len(candidates[candidates['class'] == 1]))\n",
    "print(len(candidates[candidates['class'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Classes are heaviliy unbalanced, hardly 0.2% percent are positive.\n",
    "\n",
    "The best way to move forward will be to undersample the negative class and then augment the positive class heaviliy to balance out the samples.\n",
    "\n",
    "#### Plan of attack:\n",
    "\n",
    "1. Get an initial subsample of negative class and keep all of the positives such that we have a 80/20 class distribution\n",
    "\n",
    "2. Create a training set such that we augment minority class heavilby rotating to get a 50/50 class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "positives = candidates[candidates['class']==1].index  \n",
    "negatives = candidates[candidates['class']==0].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Ok the class to get image data works\n",
    "\n",
    "Next thing to do is to undersample negative class drastically. Since the number of positives in the data set of 551065 are 1351 and rest are negatives, I plan to make the dataset less skewed. Like a 70%/30% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    13,     78,   1303,   3050,   3052,   3080,   3223,   3285,\n",
       "              3287,   3289,\n",
       "            ...\n",
       "            545928, 546205, 546372, 546400, 547498, 548674, 550171, 550334,\n",
       "            550810, 550906],\n",
       "           dtype='int64', length=1351)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1351\n",
      "6755\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "negIndexes = np.random.choice(negatives, len(positives)*5, replace = False)\n",
    "print(len(positives))\n",
    "print(len(negIndexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "candidatesDf = candidates.iloc[list(positives)+list(negIndexes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Split into test train set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X = candidatesDf.iloc[:,:-1]\n",
    "Y = candidatesDf.iloc[:,-1]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(str(X_test))\n",
    "#print(str(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create a validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5187\n",
      "1297\n",
      "1622\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_val))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of positive cases are 845\n",
      "total set size is 5187\n",
      "percentage of positive cases are 0.16290726817\n"
     ]
    }
   ],
   "source": [
    "print('number of positive cases are ' + str(Y_train.sum()))\n",
    "print('total set size is ' + str(len(Y_train)))\n",
    "print('percentage of positive cases are ' + str(Y_train.sum()*1.0/len(Y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### We will need to augment the positive dataset like mad! Add new keys to X_train and Y_train for augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6877 6877\n"
     ]
    }
   ],
   "source": [
    "tempDf = X_train[Y_train == 1]\n",
    "tempDf = tempDf.set_index(X_train[Y_train == 1].index + 1000000)\n",
    "X_train_new = X_train.append(tempDf)\n",
    "tempDf = tempDf.set_index(X_train[Y_train == 1].index + 2000000)\n",
    "X_train_new = X_train_new.append(tempDf)\n",
    "\n",
    "ytemp = Y_train.reindex(X_train[Y_train == 1].index + 1000000)\n",
    "ytemp.loc[:] = 1\n",
    "Y_train_new = Y_train.append(ytemp)\n",
    "ytemp = Y_train.reindex(X_train[Y_train == 1].index + 2000000)\n",
    "ytemp.loc[:] = 1\n",
    "Y_train_new = Y_train_new.append(ytemp)\n",
    "\n",
    "print(len(X_train_new), len(Y_train_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(X_train_new.index)\n",
    "#print(y_train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5187\n",
      "1297\n",
      "1622\n",
      "                                                seriesuid      coordX  \\\n",
      "59270   1.3.6.1.4.1.14519.5.2.1.6279.6001.132817748896...  -97.808167   \n",
      "150277  1.3.6.1.4.1.14519.5.2.1.6279.6001.182192086929...   58.990000   \n",
      "432208  1.3.6.1.4.1.14519.5.2.1.6279.6001.397522780537...  -50.307219   \n",
      "423122  1.3.6.1.4.1.14519.5.2.1.6279.6001.339882192295... -106.731000   \n",
      "344581  1.3.6.1.4.1.14519.5.2.1.6279.6001.296863826932...   67.240000   \n",
      "\n",
      "            coordY      coordZ  \n",
      "59270     3.897917 -201.030000  \n",
      "150277  -24.230000  -18.320000  \n",
      "432208  159.439740 -113.418797  \n",
      "423122 -104.468000  751.163333  \n",
      "344581  -32.590000 -107.790000  \n",
      "59270     0\n",
      "150277    0\n",
      "432208    0\n",
      "423122    0\n",
      "344581    0\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_val))\n",
    "print(len(X_test))\n",
    "print(X_train.head())\n",
    "print(Y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Prepare output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-19 01:33:58,839 INFO Dir ../../output/lung-cancer/01/ created\n"
     ]
    }
   ],
   "source": [
    "utils.mkdirs(OUTPUT_DIR, recreate=True)\n",
    "modules.logging.setup_file_logger(OUTPUT_DIR + 'out.log')\n",
    "logger.info('Dir ' + OUTPUT_DIR + ' created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create HDF5 dataset with input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(file_path, x_data, y_data):\n",
    "    logger.info('Creating dataset ' + file_path + ' size=' + str(len(x_data)))\n",
    "    with h5py.File(file_path, 'w') as h5f:\n",
    "        x_ds = h5f.create_dataset('X', (len(x_data), IMAGE_DIMS[0], IMAGE_DIMS[1], IMAGE_DIMS[2], IMAGE_DIMS[3]), chunks=(1, IMAGE_DIMS[0], IMAGE_DIMS[1], IMAGE_DIMS[2], IMAGE_DIMS[3]), dtype='f')\n",
    "        y_ds = h5f.create_dataset('Y', (len(y_data), 2), dtype='f')\n",
    "        for c, idx in enumerate(x_data.index):\n",
    "            #if(c>3): break\n",
    "            d = x_data.loc[idx]\n",
    "            filename = d[0]\n",
    "            t = Timer('Loading scan ' + str(filename))\n",
    "            scan = ctscan.CTScanMhd(INPUT_DIR, filename, coords=(d[1],d[2],d[3]))\n",
    "            pixels = scan.get_subimage(IMAGE_DIMS)\n",
    "            #add color channel dimension\n",
    "            pixels = np.expand_dims(pixels, axis=3)\n",
    "            x_ds[c] = pixels\n",
    "            y_ds[c] = [1,0]\n",
    "            if(y_data.loc[idx] == 1):\n",
    "                y_ds[c] = [0,1]\n",
    "            t.stop()\n",
    "    utils.validate_xy_dataset(file_path, save_dir=OUTPUT_DIR + 'samples/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "create_dataset(OUTPUT_DIR + 'nodules-train.h5', X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_dataset(OUTPUT_DIR + 'nodules-validate.h5', X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_dataset(OUTPUT_DIR + 'nodules-test.h5', X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-19 01:35:22,323 INFO Prepare CNN for training\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'net_nodule2d_good'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2c7256c46fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prepare CNN for training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet_nodule2d_good\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMAGE_DIMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOUTPUT_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'net_nodule2d_good'"
     ]
    }
   ],
   "source": [
    "logger.info('Prepare CNN for training')\n",
    "network = cnn.net_nodule2d_good(IMAGE_DIMS)\n",
    "model = cnn.prepare_cnn_model(network, OUTPUT_DIR, model_file=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataset_path = utils.dataset_path(INPUT_DIR, 'train', IMAGE_DIMS)\n",
    "\n",
    "with h5py.File(dataset_path, 'r') as train_hdf5:\n",
    "    X = train_hdf5['X']\n",
    "    Y = train_hdf5['Y']\n",
    "    logger.info('X shape ' + str(X.shape))\n",
    "    logger.info('Y shape ' + str(Y.shape))\n",
    "\n",
    "    dataset_path = utils.dataset_path(input_dir, 'validate', image_dims)\n",
    "    with h5py.File(dataset_path, 'r') as validate_hdf5:\n",
    "        X_validate = validate_hdf5['X']\n",
    "        Y_validate = validate_hdf5['Y']\n",
    "        logger.info('X_validate shape ' + str(X_validate.shape))\n",
    "        logger.info('Y_validate shape ' + str(Y_validate.shape))\n",
    "\n",
    "        logger.info('Starting CNN training...')\n",
    "        model.fit(X, Y, \n",
    "            validation_set=(X_validate, Y_validate), \n",
    "            shuffle=True, \n",
    "            batch_size=96, \n",
    "            n_epoch=100,\n",
    "            show_metric=True,\n",
    "            snapshot_epoch=True,\n",
    "            run_id='nodule_classifier')\n",
    "\n",
    "model.save(\"nodule-classifier.tfl\")\n",
    "logger.info(\"Network trained and saved as nodule-classifier.tfl!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with h5py.File('testdataset.h5', 'r') as test_hdf5:\n",
    "    X_test = test_hdf5['X']\n",
    "    Y_test = test_hdf5['Y']\n",
    "    Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(Y_test, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
